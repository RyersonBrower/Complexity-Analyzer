\documentclass[11pt]{article}
\usepackage{graphicx} % This lets you include figures
\usepackage{hyperref} % This lets you make links to web locations
\usepackage[margin=0.5in]{geometry}
\usepackage[rightcaption]{sidecap}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{imakeidx}
\usepackage{indentfirst}
\makeindex





\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    keywordstyle=\color{blue},
    stringstyle=\color{orange},
    commentstyle=\color{green!50!black},
    showstringspaces=false
}

%---------------------------Do Not Edit Anything Above This Line!!------------------------

% edit the line below, if needed, to change the directory name for your image files.
\graphicspath{ {./images/} }



\begin{document}

%---------------------------Edit Content in the Box to Create the Title Page--------------
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}
	   \Huge
       \textbf{Complexity Analyzer}

       \vspace{0.5cm}
       \Large
       Sprint 3 \\
       10/26/2025 \\
   \end{center}

       \vspace{1.5cm}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Name} & \textbf{Email Address} \\ \hline
Ryerson Brower         & ryerson.brower178@topper.wku        .edu \\ \hline
Aaron Downing         & aaron.downing652@topper.wku.edu         \\ \hline
\end{tabular}
\end{table}

%Latex Table Generator    
%https://www.tablesgenerator.com/     
        
\vspace{4in}

\centering        
CS 396 \\
Fall 2024\\
Project Technical Documentation

\end{titlepage}
%---------------------------Edit Content in the Box to Create the Title Page--------------


% No text here.


%---------------------------Do Not Edit Anything In This Box!!------------------------
%Table of contents and list of figures will be autogenerated by this section.
\newpage
\setcounter{page}{1}%
\cleardoublepage
\pagenumbering{gobble}
\tableofcontents
\cleardoublepage
\pagenumbering{arabic}
\clearpage
\newpage
\setcounter{page}{1}%
\cleardoublepage
\pagenumbering{gobble}
\listoffigures
\cleardoublepage
\pagenumbering{arabic}
\newpage
%---------------------------Do Not Edit Anything In This Box!!------------------------




%---------------------------Project Introduction Section------------------------------

% No text here.

\section{Introduction} %\section{} is used to create major section headers

% No text here.

%---------------------------Project Overview------------------------------------------
\subsection{Project Overview} %\subsection{} is used to create minor sections 
% 300 words
% Description of the project, what the project provides, its purpose, problems solved, and target audience.

The Algorithm Complexity Analyzer project aims to develop a microservice-based system that automates the evaluation of algorithmic efficiency through Big-O and recurrence relation analysis. The project provides a scalable platform that decomposes the complexity analysis process into independent services, each responsible for a specific function such as time complexity detection, recurrence modeling, and result presentation. By using Docker containers, the system ensures cross-platform compatibility, simple deployment, and reliable communication between services. 

The primary purpose of the project is the simplify the process of analyzing algorithmic time complexity, which is often performed manually and can often times be done wrong. Developers and students frequently struggle to derive Big-O notation from algorithmic structures. This tool addresses that challenge by automating the process and identifying recurrence relations from code snippets and giving an explanation behind each result. 

The project is designed to solve several practical and educational problems. It eliminates the need for manual complex mathematical analysis by providing automated, accurate, and explainable evaluations. It also bridges the gap between theoretical computer science problems and practical software engineering. Furthermore, by containerizing each microservice, the system supports execution, easy maintenance, and scalable updates without the risk of affecting other services. 

The target audience for this system includes, software developers, computer science students, and educators who would either like to automate their own code, or learn how to evaluate the computational complexity of algorithms efficiently. Educational institutions could integrate the tool into programming or algorithm design courses. Overall, this project delivers a platform-independent, educational, and beginner-friendly solution for algorithmic complexity evaluation. By combining automation, explanation, and scalability within a microservice architecture, it deepens our understanding of computer science theory and software engineering practice. 
%use blank lines to begin a new paragraph

%---------------------------End Project Overview---------------------------------------

% No text here.

%---------------------------Project Scope----------------------------------------------
\subsection{Project Scope}
% 350 words
% Description of all deliverables, benefits, outcomes, and work required (all tasks, costs, time, people, resources, dates/deadlines, and final deliverables date).

The Algorithmic Complexity Analyzer project focuses on developing a microservice-based tool that automates the evaluation of algorithmic efficiency through Big-) and recurrence relation analysis. The project will deliver a containerized system highlighting the microservice architecture that allows us to work on separate services independently. This ensures scalability and modularity. 

\textbf{Deliverables:}
The key deliverables include: source code for each microservice, Dockerfiles and configuration files for deployment, a working prototype of the analyzer capable of accepting code files, automated orchestration scripts using Docker Compose, and documentation detailing installation, usage, and system architecture. The final deliverable will be a deployable system that can evaluate and explain algorithmic complexity within five seconds of receiving input. 

\textbf{Work and Resources:}
The project required about two weeks of development, testing, and documentation. Tasks include system design, microservice implementation, Docker setup, and testing. Required resources include Docker Desktop, a code editor such as Visual Studio Code, a GitHub repository for version control, and access to a command line interface for deployment testing. 

\textbf{Benefits and Outcomes:}
The primary benefit of this system is automating analyzing algorithmic efficiency, which is a complex and error-prone process. The outcome is user-friendly to those who have experience using a command line interface, educational, and easily scalable. It helps students understand computational complexity by giving explanations and can assist developers in optimizing algorithms during the design phase. 


%---------------------------End Project Scope---------------------------------------

% No text here.


\subsection{Technical Requirements}


%---------------------------Functional Requirements----------------------------------------------
\subsubsection{Functional Requirements} %\subsubsection{} used to create sections for parent subsections.
% Functional requirements define what a system or software must do, specifying the desired behavior or functionality.

% List as atomic bullet points that can be tested

\begin{table}[h!]
\centering
\begin{tabular}{|l|}
\hline
\textbf{Mandatory Functional Requirements} \\ \hline
The system must support the creation and execution of automated unit tests for individual components \\
of the disaster response coordination application to ensure that each unit functions correctly in isolation.                                      \\ \hline
The framework must facilitate automated integration testing to verify that different components of the \\
application work together as intended, ensuring data flows correctly between modules.                                      \\ \hline
The system must provide a mechanism for managing test cases, allowing developers to create, modify,
and \\ organize test cases within the automation framework for easy access and execution.                                      \\ \hline
The automation framework must seamlessly integrate with existing CI/CD pipelines to enable automatic \\ triggering of test execution upon code commits or pull requests, ensuring immediate feedback
on code quality.                                           \\ \hline
The system must generate detailed reports on test results, including pass/fail rates, code coverage
metrics, and \\ historical trends, allowing teams to assess code quality and identify areas for improvement.                                           \\ \hline
\textbf{Extended Functional Requirements}  \\ \hline
Ext. Req 1                                 \\ \hline
Ext. Req 2                                 \\ \hline
Ext. Req 3                                 \\ \hline
                                           \\ \hline
                                           \\ \hline
\end{tabular}
\end{table}

% Paragraph (150 words) explaining the need and purpose for the listed Functional Requirements.
The functional requirements ensure that the test automation framework effectively supports quality assurance throughout all stages of development. Automated testing validates the functionality of individual modules, allowing early detection of errors before the final integration. Integration testing ensures that interconnected components communicate and function correctly before deployment. A centralized test case management system enables developers to easily create, modify, and organize tests. This improves accessibility and collaboration efforts. Integration with CI/CD pipelines provides continuous code validation, preventing bugs from progressing into larger problems. Finally, detailed reporting on test performance along with tracing the code through historical reports helps teams track quality trends in order to make data-driven improvement decisions. Together, these functions streamline the development process, enhancing reliability, and ensures all systems components perform as intended under real-world conditions. 


%---------------------------End Functional Requirements----------------------------------------------

% No text here.

%---------------------------Non-Functional Requirements----------------------------------------------
\subsubsection{Non-Functional Requirements}
% Non-functional requirements specify the constraints, qualities, or attributes that the system or software must possess, such as performance, security, usability, portability, fault tolerance, or reliability.

% List as atomic bullet points that can be tested

\begin{table}[h!]
\centering
\begin{tabular}{|l|}
\hline
\textbf{Mandatory Non-Functional Requirements} \\ \hline
The framework must include robust error handling and logging mechanisms to capture and report any
failures \\ or exceptions during test execution, providing developers with actionable insights for debugging
and resolution.                                      \\ \hline
The test automation framework must execute all automated tests within an average time of under 5
minutes to \\ ensure timely feedback during the development process, facilitating rapid iteration.                                      \\ \hline
The system must be designed to accommodate an increasing number of test cases and applications
without \\ degradation in performance, allowing the addition of new tests as the project evolves.                                      \\ \hline
The system must implement security best practices to protect test data and configurations, ensuring
that sensitive \\  information is not exposed during test execution or reporting processes.                                           \\ \hline
The test automation framework must provide an intuitive and streamlined interface that allows developers to \\ easily create, manage, and run test cases, ensuring a smooth onboarding process with helpful
documentation \\ and interactive tutorials to enhance user engagement and satisfaction.                                           \\ \hline
\textbf{Extended Non-Functional Requirements}  \\ \hline
Ext. Req 1                                 \\ \hline
Ext. Req 2                                 \\ \hline
Ext. Req 3                                 \\ \hline
                                           \\ \hline
                                           \\ \hline
\end{tabular}
\end{table}

% Paragraph (150 words) explaining the need and purpose for the listed Non-Functional Requirements.
The non-functional requirements focus on making the test automation framework reliable, fast, and easy to use. Strong error handling and logging are needed so developers can quickly see what went wrong during testing and fix problems without wasting time or causing delays. The tests should run in under five minutes to give quick feedback, helping teams improve efficiency and make improvements right away. The framework also needs to handle test cases and applications without slowing down, so it can grow as the project expands. Security is important to make sure that any test data, results, or settings are kept safe and private at all times. Finally, the system should be simple to use with clear instructions, examples, and tutorials that help new developers easily use the application. Altogether, these requirements ensure the network is dependable, secure, and efficient for all development teams. 
%use blank lines to begin a new paragraph




%---------------------------End Non-Functional Requirements---------------------------------------

% No text here.



%---------------------------Project Modeling and Design-------------------------------------------------

\section{Project Modeling and Design}
%describe the overall project architecture here.



\subsection{System Architecture}
%Provide a high-level diagram and description of the overall architecture of the system. This should include components such as microservices, databases, APIs, and containers (Docker).

The way we designed the architecture was as a Client-Server model where the "client" interacts with the initial microservice, which then orchestrates the remaining steps. All internal microservice communication is mandated to use TCP network sockets for API calls. The Client, Submits source code. The Complexity Analysis Service, Ingests the code, identifies the core algorithm structure, and models the recurrence relation, this uses one of the docker containers. The Recurrence Relation Service, Solves the recurrence relation and computes the Big-O notation, this also uses a docker container. The Result Presentation Service, Generates the final output (Big-O and explanation) and serves it to the client, this is the last one that uses a docker container. The last one is Docker Compose, which automates the building, starting, and connecting of the three microservice containers. The flow of the data and usage goes like this. Client submits code to the Complexity Analysis Service (Input). Complexity Analysis Service sends the generated recurrence relation to the Recurrence Relation Service. Recurrence Relation Service sends the calculated Big-O notation and solution steps to the Result Presentation Service. Result Presentation Service formats the final result and sends it back to the Client (Output).

\subsection{Defined Microservices}
%Describe each microservice or component in detail, including its role, input/output, and interactions with other services.

Our first microservice is the Complexity Analysis Service (CAS). Its role is to be the entry point of the pipeline. It parses the input code snippet (e.g., using Abstract Syntax Trees or code analysis techniques) to mathematically model its runtime behavior, primarily focusing on loops, recursion, and conditional branches, to derive a formal recurrence relation. The next one is the Recurrence Relation Service (RRS). Its role is to solve the provided recurrence relation to determine the asymptotic time complexity. It must implement algorithms (like the Master Theorem or expansion/substitution methods) to handle standard forms, especially divide-and-conquer recurrences. The last one is the Result Presentation Service (RPS). Its role is to finalize the process by compiling the Big-O notation and the detailed analysis explanation into a single, highly readable and structured format for the user. It ensures the Big-O is prominent and the explanation is detailed.

\subsection{Development Considerations}
%Discuss any key design decisions, trade-offs, or challenges faced during the architecture design. Explain why certain tools or frameworks were chosen.
There wasn't really many considerations we took into account. We just wanted to finish it as fast as possible and also we already had a pretty good idea of what we wanted to do so we just stuck with it. 


%---------------------------End Project Modeling and Design-------------------------------------------------

% No text here.


%---------------------------Implementation Approach-------------------------------------------------

\section{Implementation Approach}

\subsection{Software Process Model}
%define your approach to development using a specific software process model.

Our approach to this project was to just get it done as fast as possible. So we did it in one big sprint. We went and did all the coding first and got it done with and working. Then we went on to the documentation. 

\subsection{Key Algorithms and Techniques Used}
The system employs a layered analytical strategy spanning regular expression matching and symbolic math:


Service: Complexity Analysis Service (CAS), Algorithm: Naive Recurrence Detection (Regex). Uses Python's re module to perform simple, pattern-based recognition. It specifically targets the divide-and-conquer structure T(n)=aT(n/b)+f(n) and provides basic hints for for/while loops. This is a fast, but limited, form of static analysis.

Service: Recurrence Relation Service (RRS), Algorithm: Master Theorem Implementation. Implements the logic of the Master Theorem to solve recurrences of the form $T(n)=aT(n/b)+f(n)$. The service calculates the threshold $\log_b a$ and compares it to $f(n)$ (simplified as $n^c$) to determine the final $\Theta$ bound (Cases 1, 2, and 3).

Service: Result Presentation Service (RPS), Algorithm:Result Aggregation and Formatting. This service acts as an API Gateway/Aggregator, consuming the raw analysis from the CAS (which, in turn, includes the RRS result) and compiling the final, structured output, ensuring the Big-O notation is prominent.

\subsection{Microservices and Docker Usage}
The project is structured around three independent microservices, each running within its own Docker container:

Service Name: analyzer (CAS), Code name: analyzer.py. Its role is to Receive code, detects recurrence, and orchestrates the call to the RRS. On port 5000.

Service Name: recurrence (RRS), Code name: recurrence.py. It solves the Master Theorem recurrence. On port 5001

Service Name: presenter (RPS), Code name: presenter.py. It is the Final aggregation and output formatting.On port 5002

Docker Usage: Each service uses a minimal python:3.11-slim base image defined in its dedicated Dockerfile. This ensures a lightweight, reproducible environment, satisfying the non-functional requirement for platform independence.

\subsection{Container Network Communication Setup and Testing}
Communication Setup:

For the first step of the setup. The protocols are all inter-service communication uses the Hypertext Transfer Protocol (HTTP), which operates over TCP network sockets, fulfilling Functional Requirement. 
The second part of the setup is the Service Discovery. Here the key is the use of service names defined in docker-compose.yml: The analyzer service uses http://recurrence:5001/solve to reach the RRS. The presenter service uses http://analyzer:5000/analyze to reach the CAS. Docker Compose's internal networking and DNS provide this name resolution. For our network testing we made a simple curl command to http://localhost:5002/present (which should trigger the entire internal pipeline) validates the end-to-end communication path. Then we did time constraint test. The requests library calls include a timeout (5 or 6 seconds), aligning with Non-Functional Requirement 4 to complete the analysis quickly.

\subsection{Infrastructure as Code approach}



%---------------------------End Implementation Approach-------------------------------------------------



% No text here.

%---------------------------Software Product Testing Section-------------------------------------
\section{Software Testing and Results}



%---------------------------Software Testing Plan Template-------------------------------------

\subsection{Software Testing with Visualized Results}
%Each of the testing levels (unit, Integration, System, Acceptance) should use the following test plan template.
\subsection{Unit Testing Plan}



\textbf{Test Plan Identifier:} UT-001  
Provides a unique identifier for this test plan. This plan focuses on unit testing the \texttt{naive\_data\_recurrence} function in the Complexity Analyzer software.

\textbf{Introduction:}  
This unit test plan verifies that the recurrence detection module correctly identifies simple loops, divide-and-conquer recurrences, and unknown code patterns. The objective is to detect functional errors early, ensure reliable classification, and support future regression testing.

\textbf{Test item:}  
The Software Under Test (SUT) is the function \texttt{naive\_data\_recurrence} in \texttt{analyzer.app}. It takes code snippets or recurrence equations as input and outputs a dictionary indicating the recurrence type and parameters.

\textbf{Features to test/not to test:}  
\textbf{In scope:} Correct identification of loops, divide-and-conquer recurrences, and unknown patterns. Validation of returned dictionary keys and values.  
\textbf{Out of scope:} Advanced recurrence analysis, multi-language support, and performance testing. Excluded due to low priority for unit-level testing.

\textbf{Approach:}  
Automated unit tests using \texttt{pytest} will supply input strings to \texttt{naive\_data\_recurrence} and assert expected outputs. Functional correctness is the focus. Manual intervention is not required, and performance or security testing is excluded.

\textbf{Test deliverables:}  
Deliverables include unit test scripts, expected results, actual test results, and pytest-generated reports. Documentation of test cases and execution logs will be maintained for traceability.

\textbf{Item pass/fail criteria:}  
Tests pass if the function correctly identifies the recurrence type and parameters. Any mismatch between expected and actual output constitutes a failure. All test cases must pass for the module to be considered correct.

\textbf{Environmental needs:}  
Python 3.13.1 with pytest 8.4.2 installed. Access to the \texttt{analyzer.app} module. No network or external services are required.

\textbf{Responsibilities:}  
Developers create and maintain test scripts. QA engineers execute tests, record results, and report issues. Both teams review failures and coordinate fixes.

\textbf{Staffing and training needs:}  
Testers must know Python, pytest, and the SUT module. Minimal training is required to understand recurrence types and testing procedures.

\textbf{Schedule:}  
Test cases are run alongside development.

\textbf{Risks and Mitigation:}  
Risks include incomplete coverage if new recurrence types are added without updating tests, or misclassification errors. Mitigation involves regular test updates, automated regression tests, and code reviews. Changes to the SUT interface require corresponding test updates.

\textbf{Approvals:}  
Team approval required.

\subsection{Integration Testing Plan}

\textbf{Test Plan Identifier:} IT-001  
Provides a unique identifier for this test plan. This plan focuses on integration testing the Flask API endpoints of the Complexity Analyzer software.

\textbf{Introduction:}  
This integration test plan verifies that the `/analyze` API endpoint correctly handles valid and invalid requests. The objective is to ensure proper request processing, response formatting, and error handling when interacting with the recurrence detection module.

\textbf{Test item:}  
The Software Under Test (SUT) is the Flask app in \texttt{analyzer.app}. The `/analyze` endpoint receives JSON payloads containing code snippets, invokes \texttt{naive\_detect\_recurrence}, and returns a structured JSON response with recurrence type and Big-O hints or error messages.

\textbf{Features to test/not to test:}  
\textbf{In scope:} Correct response for valid code submissions, appropriate error responses for invalid/empty payloads, and proper JSON formatting.  
\textbf{Out of scope:} Authentication, rate limiting, performance under high load, and non-JSON input handling. Excluded due to focus on core functionality and unit integration.

\textbf{Approach:}  
Automated integration tests using Flask’s test client will simulate POST requests to `/analyze` and validate responses. Tests focus on functional correctness, status codes, and response content. Manual testing is not required. No performance or security tests are included in this plan.

\textbf{Test deliverables:}  
Deliverables include integration test scripts, input payloads, expected outputs, actual responses, and test reports generated by pytest or CI tools. Execution logs and documentation will be maintained for traceability.

\textbf{Item pass/fail criteria:}  
Tests pass if the API returns the correct status code and response JSON for each input. Failures occur when response structure or content deviates from expected output. All test cases must pass for the endpoint to be considered functionally correct.

\textbf{Environmental needs:}  
Python 3.13.1 with Flask and pytest installed. Access to the \texttt{analyzer.app} module. Local development environment or CI server capable of running Flask test clients. No external services required.

\textbf{Responsibilities:}  
Developers write and maintain integration test scripts. QA engineers execute tests, record results, and report discrepancies. Both teams collaborate to address failures, review logs, and ensure reliability of API endpoints.

\textbf{Staffing and training needs:}  
Team members should be familiar with Python, Flask, pytest, and JSON handling. Minimal training required for understanding API endpoints and response validation for integration testing.

\textbf{Schedule:}  
Integration tests are run alongside development to ensure everything is running smoothly.

\textbf{Risks and Mitigation:}  
Risks include incomplete coverage if new endpoint features are added without updating tests or changes to JSON response structure. Mitigation includes maintaining test scripts alongside API updates, automated regression testing, and code review. API interface changes require corresponding test adjustments.

\textbf{Approvals:}  
Team approval required.


\subsection{System Testing Plan}

\textbf{Test Plan Identifier:} ST-001  
Provides a unique identifier for this test plan. This plan focuses on system-level testing of the local Flask app of the Complexity Analyzer software.  

\textbf{Introduction:}  
This system test plan verifies that the Flask application responds correctly to `/analyze` requests. The objective is to ensure functional correctness of the complete system, including request processing, response structure, and handling of valid, empty, and unknown inputs.  

\textbf{Test item:}  
The Software Under Test (SUT) is the Flask application in \texttt{analyzer.app}. The `/analyze` endpoint accepts JSON payloads, invokes the recurrence detection logic, and returns structured JSON responses including recurrence type and Big-O hints.  

\textbf{Features to test/not to test:}  
\textbf{In scope:} Correct HTTP responses for valid code submissions, empty requests, and unknown code patterns. Verification of status codes and JSON structure.  
\textbf{Out of scope:} Docker deployment, external network communication, authentication, and high-load performance. These are excluded because this test focuses on local system functionality.  

\textbf{Approach:}  
Automated system tests using Flask’s built-in test client will simulate POST requests to `/analyze`. Tests validate status codes, JSON keys, and response content. Functional correctness is prioritized. No manual testing or performance testing is included.  

\textbf{Test deliverables:}  
Deliverables include the system test scripts, input payloads, expected outputs, actual responses, and pytest-generated reports. Logs and documentation of test results will be maintained for traceability and regression testing.  

\textbf{Item pass/fail criteria:}  
Tests pass if the Flask app returns correct status codes and response JSON for all input scenarios. Failures occur when response content, structure, or status code differs from expected results. All tests must pass for the system to be considered correct.  

\textbf{Environmental needs:}  
Python 3.13.1 with Flask and pytest installed. Access to the local \texttt{analyzer.app} module. No external services or network connections are required.  

\textbf{Responsibilities:}  
Developers create and maintain system test scripts. QA engineers execute tests, document results, and report failures. Both teams review and address issues to ensure correct system behavior.  

\textbf{Staffing and training needs:}  
Team members should be familiar with Python, Flask, pytest, and HTTP request/response handling. Minimal training is needed to understand endpoint behavior and system-level testing procedures.  

\textbf{Schedule:}  
As soon as possible. Automated execution enables repeated testing after code changes.  

\textbf{Risks and Mitigation:}  
Risks include incomplete coverage if new API features are added without updating tests, or misclassification in the recurrence detection module. Mitigation includes maintaining test scripts alongside code updates, automated regression testing, and code review. Changes to the API interface require corresponding updates to system tests.  

\textbf{Approvals:}  
Team approval required.


%---------------------------Software Testing Plan Template-------------------------------------





%---------------------------End Software Product Testing Section-------------------------------------


% No text here.



%---------------------------Conclusion Section-------------------------------------
\section{Conclusion}
%200 words
%Concluding remarks that summarizes the purpose and outcomes of the technical document.  Discussion of short comings and future work.
Overall, this was a very useful project to work on. We had very limited knowledge using Docker and microservices before this project. Although we are nowhere near experts in the space yet, we gained footing in containerization practices. The project itself is very useful and fortified our understanding on algorithm analysis as we developed the product. To make it easier on ourselves, we stuck to a command line interface. In past projects, we have tried to stay away from this, but for this one the most important part was making sure it was calculating things effectively and efficiently. Going forward, if we had more time to work on this project or were to continue to work on it after this sprint is done. I believe we could create a simple web-based or graphical user interface. Then we could make the system simpler to use and be able to insert more complex code. As of now, it is pretty basic but we really enjoyed the direction it was going as we went through the tough learning curve. 

%---------------------------End Conclusion Section-------------------------------------


% No text here.



%---------------------------Appendix Section-------------------------------------------
\section{Appendix}

\subsection{Software Product Build Instructions}

To continue development of the Algorithm Complexity Analyzer on a new computer, follow these steps:

\textbf{Step 1: Copy the Project Folder}

Copy the entire project directory to the new machine, ensuring all subfolders and files are preserved.  


\textbf{Step 2: Install Prerequisites}

Install the following on the new machine:

- Docker Desktop  
- PowerShell 7 (optional)  
- VS Code or another code editor (optional)

\textbf{Step 3: Verify Docker}

Open PowerShell and run:

\begin{verbatim}
docker --version
docker compose version
\end{verbatim}

\textbf{Step 4: Start the Microservices}

Navigate to the project folder:

\begin{verbatim}
cd "C:\path\to\algorithm-analyzer"
\end{verbatim}

Then run one of the following:

\begin{verbatim}
docker compose up --build
# or run in background
docker compose up -d --build
# or use the script
.\run.ps1
\end{verbatim}

\textbf{Step 5: Test the Setup}

Send a test request:

\begin{verbatim}
Invoke-RestMethod -Uri "http://localhost:5002/present" `
  -Method Post `
  -Headers @{ "Content-Type" = "application/json" } `
  -Body '{"code":"T(n) = 2T(n/2) + n"}' | ConvertTo-Json -Depth 10
\end{verbatim}

\textbf{Step 6: Stop the Microservices}

\begin{verbatim}
docker compose down
\end{verbatim}

---

\subsection{Software Product User Guide}

The Algorithm Complexity Analyzer allows users to submit algorithm code and receive time complexity analysis. 

\subsubsection{General User}

General users can:

\textbf{Capabilities:}
- Submit algorithm code snippets in standard programming syntax.
- Receive analysis results including:
  - Big-O notation of the algorithm
  - Explanation of the analysis
  - Recurrence solution (if applicable)

\textbf{Example:} To submit code, send a POST request to the Presentation microservice API:

\begin{verbatim}
POST http://localhost:5002/present
Body: {"code":"T(n) = 2T(n/2) + n"}
\end{verbatim}

The response will contain JSON with the `big\_o` value, explanation, and recurrence solution.

\-\-\-

\subsubsection{Administrative User}

Administrative users manage the microservices and development environment. Their responsibilities include:

\textbf{Starting and Stopping Docker Containers:}

\begin{verbatim}
docker compose up -d
docker compose down
\end{verbatim}

\textbf{Updating or Rebuilding Services After Code Changes:}

\begin{verbatim}
docker compose up --build
\end{verbatim}

\textbf{Monitoring Container Status:}

\begin{verbatim}
docker ps
\end{verbatim}

Administrators can also modify or extend microservices for new algorithms or features.





\subsection{Source Code with Comments}
%Include in this section all final source code for the product.  Label each file with headings such as, C.1 file1.c, C.2 file2.c, C.3 file1.py, etc.  All source code should be effectively commented.


\noindent\textbf{File:} \texttt{analyzer/app.py}

\begin{lstlisting}[language=Python, caption={}, label={lst:analyzer-app}]
from flask import Flask, request, jsonify
import requests
import re

app = Flask(__name__)
RECURRENCE_SERVICE = "http://recurrence:5001/solve"

def naive_detect_recurrence(code: str):
    s = code.replace(" ", "")
    m = re.search(r"T\(n\)\s*=\s*(\d+)T\(n/(\d+)\)\+(.+)", s)
    if m:
        a, b, f = m.groups()
        return {"type": "divide-and-conquer", "a": int(a), "b": int(b), "f": f}
    if re.search(r"for\s+\w+\s+in\s+range|while\s*\(", code):
        return {"type": "loop", "hint": "may be O(n) or O(n^2) depending on nested loops"}
    return {"type": "unknown"}

@app.route("/analyze", methods=["POST"])
def analyze():
    payload = request.json or {}
    code = payload.get("code", "")
    if not code:
        return jsonify({"error": "no code provided"}), 400

    rec = naive_detect_recurrence(code)
    result = {"recurrence_detected": rec}

    if rec.get("type") == "divide-and-conquer":
        try:
            resp = requests.post(RECURRENCE_SERVICE, json={
                "a": rec["a"], "b": rec["b"], "f": rec["f"]
            }, timeout=5)
            result["recurrence_solution"] = resp.json()
        except Exception as e:
            result["recurrence_solution"] = {
                "error": "could not reach recurrence service",
                "detail": str(e)
            }

    if rec.get("type") == "loop":
        result["big_o_hint"] = "O(n) or O(n^2) - check nested loops."
    elif rec.get("type") == "divide-and-conquer":
        result["big_o_hint"] = "Use recurrence solution returned above."
    else:
        result["big_o_hint"] = "unknown"

    return jsonify(result)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
\end{lstlisting}




\noindent\textbf{File:} \texttt{recurrence/app.py}

\begin{lstlisting}[language=Python, caption={}, label={lst:recurrence-app}]
from flask import Flask, request, jsonify
import math

app = Flask(__name__)

def master_theorem(a, b, f_str):
    try:
        a = int(a); b = int(b)
    except:
        return {"error": "a and b must be integers"}
    log_term = math.log(a, b)
    import re
    m = re.search(r"n\^(\d+)", f_str)
    if m:
        c = int(m.group(1))
        if abs(c - log_term) < 1e-6:
            return {"theta": f"n^{c}", "case": "2 (f(n) = Theta(n^{log_b a}))"}
        elif c < log_term:
            return {"theta": f"n^{log_term:.3f}", "case": "1 (f(n) = O(n^{log_b a - eps}))"}
        else:
            return {"theta": f"f(n) dominates (approx n^{c})", "case": "3 (f(n) = Omega(n^{log_b a + eps}))"}
    return {"theta": f"n^{log_term:.3f} (master theorem baseline)", "note": "f(n) not parsed; returned baseline"}

@app.route("/solve", methods=["POST"])
def solve():
    data = request.json or {}
    a = data.get("a")
    b = data.get("b")
    f = data.get("f", "unknown")
    if a is None or b is None:
        return jsonify({"error": "missing a or b"}), 400
    sol = master_theorem(a, b, str(f))
    return jsonify({"a": a, "b": b, "f": f, "solution": sol})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)
\end{lstlisting}





\noindent\textbf{File:} \texttt{presentation/app.py}

\begin{lstlisting}[language=Python, caption={}, label={lst:presentation-yeayea}]
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)
ANALYZER_SERVICE = "http://analyzer:5000/analyze"

@app.route("/present", methods=["POST"])
def present():
    payload = request.json or {}
    code = payload.get("code")
    if not code:
        return jsonify({"error": "no code provided"}), 400

    try:
        # Step 1: Send code to Analyzer
        resp = requests.post(ANALYZER_SERVICE, json={"code": code}, timeout=6)
        analysis = resp.json()
    except Exception as e:
        return jsonify({"error": "could not reach analyzer", "detail": str(e)}), 500

    # Step 2: Extract recurrence solution if available
    recurrence_solution = analysis.get("recurrence_solution", {})
    solution_data = recurrence_solution.get("solution", {})

    # Step 3: Fill big_o using recurrence solution if available
    if solution_data.get("theta"):
        big_o = solution_data["theta"]
    else:
        big_o = analysis.get("big_o_hint", "unknown")

    formatted = {
        "big_o": big_o,
        "explanation": "The recurrence was analyzed using the Recurrence service. See recurrence_solution for step-by-step details.",
        "recurrence_analysis": recurrence_solution
    }

    return jsonify({"analysis": formatted, "raw": analysis})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5002)
\end{lstlisting}




%---------------------------End Appendix Section-------------------------------------------














%example image:  uncomment to show usage
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=1\textwidth]{images/Add_non-music.png}
%    \caption{This is how you add non-music items.}
%    \label{fig16}
%\end{figure}


%example links:  uncomment to show usage.
%\url{https://www.youtube.com}
%\href{https://www.wku.edu/}{WKU Homepage}
%\footnote{You can put the link in a footnote like this.}

% Anything to the right of a percent sign will be ignored by LaTeX.
% You can use this to put notes to yourself.  



\end{document}
